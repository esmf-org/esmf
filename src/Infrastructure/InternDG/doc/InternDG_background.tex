% $Id: InternDG_background.tex,v 1.1 2006/03/23 01:29:43 theurich Exp $

\section{Background}

Scalable implementations of finite-difference codes are generally
based on decomposing the model domain into subdomains that are
distributed among processors. These domains may then be obliged to
share data at their boundaries if data dependencies are merely
local, or may need to acquire information from the global domain if
there are extended data dependencies, as in the spectral transform, or
in elliptic solvers. The \emph{distributed grid} 
is a category within ESMF used for expressing, and
performing operations that involve, dependencies among data
distributed across processors.

\subsection{Scope}

The discrete representation of data fields within a model component
begins with the definition of \emph{physical grid} associated with the
data. The data fields thus get defined as arrays, which are then
distributed among processors. The indices associated with array
locations in each dimension thus define a global \emph{index space}. The
distributed grid encompasses all the ESMF infrastructure operations
associated with the index-space representation of data. Operations
requiring knowledge of actual physical locations and distances between
locations belong to the physical grid, and are found in the
documents associated with it.

\subsection{Location}

The gridded component, physical grid and distributed grid
elements form a closely-linked conceptual chain. The distributed grid also
provides the interface to the machine layer where the scheduling,
communication and memory management primitives reside. Applications
should rarely need to reach beyond the distributed grid layer for
direct invocation of the communication primitives.


\subsection{Summary}

A gridded component is associated with one or more global physical
grids.  Distributing a global physical grid across PEs generates a
physical grid element, which is associated with a a single
distributed grid element, distributed across some or all of the PEs
associated with the component. The distributed grid has operations to
define domain decompositions on the pelist, and given user-specified
data dependencies, it can define topologies, i.e. connectivities on
the pelist for scheduling communication. It contains all the
operations for sharing data according to those dependencies.  This
includes familiar operations like the halo update and data transpose.
It can perform global reductions (sum, max, min, maxloc, minloc) on
distributed data.  The sum has a bitwise exact option. It can create a
copy of the global data on one or more PEs, and scatter a global array
across a PEList.

The distributed grid will perform sign flips, vector component interchanges,
and redundancy checks as needed on certain types of grids (e.g
tripolar grid and cubed-sphere).

Operations within the distributed grid do not include those which are
potentially dependent on grid metrics. For instance, certain kinds of
averaging operations are extremely simple on regular grids. These are
metric-dependent on irregularly spaced grids, and are hence considered
to belong to the physical grid.

