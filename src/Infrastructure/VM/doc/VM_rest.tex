% $Id$

%\subsubsection{Restrictions and Future Work}

\begin{enumerate}

\item {\bf Only array section syntax that leads to contiguous sub sections is supported}. The source and destination arguments in VM communication calls must reference contiguous data arrays. Fortran array sections are not guaranteed to be contiguous in all cases.

\item {\bf Non-blocking {\tt Reduce()} operations {\em not} implemented.} None of the reduce communication calls have an implementation for the non-blocking feature. This affects:
\begin{itemize}
\item {\tt ESMF\_VMAllFullReduce()},
\item {\tt ESMF\_VMAllReduce()},
\item {\tt ESMF\_VMReduce()}.
\end{itemize}

\item {\bf Limitations when using {\tt mpiuni} mode.} In {\tt mpiuni} mode non-blocking communications are limited to one outstanding message per source-destination PET pair. Furthermore, in {\tt mpiuni} mode the message length must be smaller than the internal ESMF buffer size.

\item {\bf Alternative communication paths not accessible.} All user accessible VM communication calls are currently implemented using MPI-1.2. VM's implementation of alternative communication techniques, such as shared memory between threaded PETs and POSIX IPC between PETs located on the same single system image, are currently inaccessible to the user. (One exception to this is the {\tt mpiuni} case for which the VM automatically utilizes a shared memory path.)

\item {\bf Data arrays in VM comm calls are {\em assumed shape} with rank=1.} Currently all dummy arrays in VM comm calls are defined as {\em assumed shape} arrays of rank=1. The motivation for this choice is that the use of assumed shape dummy arrays guards against the Fortran copy in/out problem. However it may not be as flexible as desired from the user perspective. Alternatively all dummy arrays could be defined as {\em assumed size} arrays, as it is done in most MPI implementations, allowing arrays of various rank to be passed into the comm methods. Arrays of higher rank can be passed into the current interfaces using Fortran array syntax. This approach is explained in section \ref{vm_higherrank}.

\item {\bf Limitations when using VMEpoch.} Using a blocking collective call (e.g. {\tt ESMF\_VMBroadcast()}, the {\tt MPI\_Bcast()} used by {\tt ESMF\_InfoBroadcast()}, etc.) within the region enclosed by {\tt ESMF\_VMEpochEnter()} and {\tt ESMF\_VMEpochExit()} will result in a deadlock. 

  
\end{enumerate}


