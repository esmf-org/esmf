%===============================================================================
% CVS $Id$
%===============================================================================

\subsection{Testing and Validation}
\label{sec:testing}

ESMF software is subject to the following tests:
\begin{enumerate}
\item Unit tests, which are simple per-class tests.
\item Testing Harness, parameter space spanning tests similar to the unit tests
\item System tests, which generally involve inter-component interactions.
\item Use test cases (UTCs), which are tests at realistic problem
sizes (e.g., large data sets, processor counts, grids).
\item Examples that range from simple to complex.
\item Beta testing through preliminary releases.
\end{enumerate}
Unit tests, system tests, and examples are distributed with the
ESMF software.  UTCs, because of their size, are 
stored and distributed separately.  Tests are run nightly,
following a weekly schedule, on a wide variety of platforms.  
Beta testing of ESMF software is done by providing an Internal Release
to customers three months before public release.  

The ESMF team keeps track of test coverage on a per-method basis.
This information is on the {\bf Metrics} page under the {\bf Development}
link on the navigation bar.

Testing information is stored on a {\bf Test and Validation} web page,
under the {\bf Development} link on the ESMF 
web site.  This web page includes:
\begin{itemize}
\item separate web pages for each system test and UTC;
\item links to the {\it Developer's Guide}, GitHub Tracker, Requirements 
Spreadsheet, and any other pertinent information; and
\item separate web page for automated regression test information and results.
\end{itemize}

The ESMF is designed to run on several target platforms, in different 
configurations, and is required to interoperate with many combinations 
of application software. Thus our test strategy includes the following.

\begin{itemize}
\item Tests are executed on as many target platforms as possible. 
\item Tests are executed on a variety of programming paradigms
(e.g pure shared memory, pure distributed memory and a mix of both).
\item Tests are executed in multiple configurations (e.g. uni-processor,
multi-processor).
\item The result of each test is a {\tt PASS/FAIL}.  
\item In some cases, for floating point comparisons, an epsilon value
will be used.
\item Tests are implemented for each language interface that is 
supported.
\end{itemize}

\subsubsection{Unit Tests}

Each class in the framework is associated with a suite of unit tests.
Typically the unit tests are stored in one file per class, and are
located near the corresponding source code in a test directory.  The 
framework {\tt make} system will have an option to build and run unit tests.
The user has the option of building either a "sanity check" type test
or an exhaustive suite. The exhaustive tests include tests of many 
functionalities and a variety of valid and invalid input values. The sanity 
check tests are a minimum set of tests to indicate whether, for example, the 
software has been installed correctly. It is the responsibility of the 
software developer to write and execute the unit tests. Unit tests 
are distributed with the framework software.

To achieve adequate unit testing, developers shall attempt to meet the following goals. 

\begin{itemize}
\item Individual procedures will be evaluated with at least one unit
test function.  However, as many test functions as necessary will be
implemented to assure that each procedure works properly.  
\item Developers should unit test their code to the degree possible  
before it is checked into the repository.  It is assumed that 
developers will use stubs as necessary.
\item Variables are tested for acceptable range and precision.
\item Variables are tested for a range of valid values, including boundary
values.
\item Unit tests should verify that error handling works correctly.
\end{itemize}

\paragraph{Writing Unit Tests}

Unit tests usually test a single argument of a method to make it easier to
identify the bug when a unit test fails.
There are several steps to writing a unit test.
First, each unit test must be labeled with one of the following tags:
\begin{itemize}
\item NEX\_UTest - This tag signifies a non-exhaustive test. These tests are always run and
are considered to be sanity tests they usually consist of creating and destroying a specific class.
\item EX\_UTest - This tag signifies an exhaustive unit test. These tests are more rigorous and
are run when the ESMF\_EXHAUSTIVE environmental variable is set to ON. These unit test must be between the \#ifdef ESMF\_EXHAUSTIVE
and \#endif definitions in the unit test file.
\item NEX\_UTest\_Multi\_Proc\_Only - These are non-exhaustive multi-processor unit tests that will not be
run when the run\_unit\_tests\_uni or unit\_test\_uni targets are specified.
\item EX\_UTest\_Multi\_Proc\_Only - These are exhaustive multi-proccesor unit tests that will not be
run when the run\_unit\_tests\_uni or unit\_tests\_uni targets are specified.
\end{itemize}
Note that when the NEX\_UTest\_Multi\_Proc\_Only or EX\_UTest\_Multi\_Proc\_Only tags are used, all the unit tests in
the file must be labeled as such. You may not mix these tags with the other tags. In addition, verify that the makefile
does not allow the unit tests with these tags to be run uni.

Second, a string is specified describing the test, for example:
\begin{verbatim}

	write(name, *) "Grid Destroy Test"

\end{verbatim}
Third, a string to be printed when the test fails is specified, for example:
\begin{verbatim}

	write(failMsg, *) "Did not return ESMF_SUCCESS"

\end{verbatim}
Fourth, the ESMF\_Test subroutine is called to determine the test results, for example:
\begin{verbatim}

	call ESMF_Test((rc.eq.ESMF_SUCCESS), name, failMsg, result, ESMF_SRCLINE)

\end{verbatim}
The following two tests are good examples of how unit tests should be written.
The first test verify that getting the attribute count from a Field returns ESMF\_SUCCESS, while
the second verifies the attribute count is correct. These two tests could be combined into one
with a logical AND statement when calling ESMF\_Test, but breaking the tests up allows you
to identify the source of the bug immediately.
\begin{verbatim}

      !------------------------------------------------------------------------
      !EX_UTest
      ! Getting Attrubute count from a Field
      call ESMF_FieldGetAttributeCount(f1, count, rc=rc)
      write(failMsg, *) "Did not return ESMF_SUCCESS"
      write(name, *) "Getting Attribute count from a Field "
      call ESMF_Test((rc.eq.ESMF_SUCCESS), name, failMsg, result, ESMF_SRCLINE)

      !------------------------------------------------------------------------
      !EX_UTest
      ! Verify Attribute Count Test
      write(failMsg, *) "Incorrect count"
      write(name, *) "Verify Attribute count from a Field "
      call ESMF_Test((count.eq.0), name, failMsg, result, ESMF_SRCLINE)

      !------------------------------------------------------------------------

\end{verbatim}


Sometimes a unit test is written expecting a subset of the processors to fail the test. To
handle this case, the unit test must verify results from each processor as in the unit test below:
\begin{verbatim}


    !------------------------------------------------------------------------
    !EX_UTest
    ! Verify that the rc is correct on all pets.
    write(failMsg, *) "Did not return FAILURE  on PET 1, SUCCESS otherwise"
    write(name, *) "Verify rc of a Gridded Component Test"
    if (localPet==1) then
      call ESMF_Test((rc.eq.ESMF_FAILURE), name, failMsg, result, ESMF_SRCLINE)
    else
      call ESMF_Test((rc.eq.ESMF_SUCCESS), name, failMsg, result, ESMF_SRCLINE)
    endif

    !------------------------------------------------------------------------

\end{verbatim}

Some tests may require that a loop be written to verify multiple results. The following is
an example of how a single tag, NEX\_UTest, is used instead of a tag for each loop iteration.

\begin{verbatim}

 !-----------------------------------------------------------------------------
  !NEX_UTest
  write(name, *) "Verifying data in Array via Fortran array pointer access"
  write(failMsg, *) "Incorrect data detected"
  looptest = .true.
  do i = -12, -6
    j = i + 12 + lbound(fptr, 1)
    print *, fptr(j), fdata(i)
    if (fptr(j) /= fdata(i)) looptest = .false.
  enddo
  call ESMF_Test(looptest, name, failMsg, result, ESMF_SRCLINE)
  !-----------------------------------------------------------------------------

\end{verbatim}

\paragraph{Analyzing unit test results }
When unit test are run, a Perl script prints out the test results as shown in
Section "Running ESMF Unit Tests" in the ESMF User's Guide. To print out the test results,
the Perl script must determine the number of unit tests in each test file and the number of
processors executing the unit test. It determines the number of tests by counting the
EX\_UTest, NEX\_UTest, EX\_UTest\_Multi\_Proc\_Only, or NEX\_UTest\_Multi\_Proc\_Only 
tags in the test source file whichever is appropriate for the test being run. 
To determine the number of processors, it counts the number of "NUMBER\_OF\_PROCESSORS" strings in the
unit test output Log file. The script then counts the number of PASS and FAIL strings in the
test Log file.
The Perl script first divides the number of PASS strings by the number of processors. If the
quotient is not a whole  number then the script concludes that the test crashed. If the quotient
is a whole number, the script then divides the number of FAIL strings by the number of processors.
The sum of the two quotients must equal the total number of tests, if not the test is marked
as crashed.



\paragraph{Disabling unit tests }
Sometimes in the software development process it becomes necessary to disable one or more unit tests.
To disable a unit test, two lines need to be modified. First, the line calling "ESMF\_Test" must be commented out.
Second, the NEX\_UTest, EX\_UTest, NEX\_UTest\_Multi\_Proc\_Only and EX\_UTest\_Multi\_Proc\_Only tags must be modified
so that they are not found by the Perl script that analyzes the test results.
The recommended way to modify these tags is to replace the first underscore with "\_disable\_", thus NEX\_UTest becomes
NEX\_disable\_UTest. 

\paragraph{Benchmarking Unit Tests}
Benchmark testing is included in the ESMF regression tests to detect any unexpected
change in the performance of the software. This capability is available to developers.
Developers can run the unit tests and save their execution
times to be used as a benchmark for future unit test runs.

The following section now appears in the output of "make info".

\begin{verbatim}
 
--------------------------------------------------------------
 * ESMF Benchmark directory and parameters *
ESMF_BENCHMARK_PREFIX:    ./DEFAULTBENCHMARKDIR
ESMF_BENCHMARK_TOLERANCE: 3%
ESMF_BENCHMARK_THRESHOLD_MSEC: 500
 
--------------------------------------------------------------

\end{verbatim}

The steps for using the benchmarking test tool are as follows:

\begin{sloppypar}
\begin{itemize}
\item After building the unit tests, execute "make run\_unit\_tests" and 
verify that all tests pass. It not recommended that failing tests be benchmarked.
\item Set "BENCHMARKINSTALL = YES" and execute "make run\_unit\_tests\_benchmark".
This will cause the unit tests stdout files to be copied to the "DEFAULTBENCHMARKDIR"
directory. The elapsed times of these unit tests are the now the benchmark. 
The default of DEFAULTBENCHMARKDIR is \$ESMF\_DIR/DEFAULTBENCHMARKDIR.
It is advised that the benchmarking directory be outside the ESMF structure, to allow
the developer to benchmark different versions of the software. The benchmark directory
can be changed by setting ESMF\_BENCHMARK\_PREFIX.
\item Run the unit test a second time.
\item To compare the elapsed times of this unit test run to the benchmared run, set
"BENCHMARKINSTALL = NO", and execute "make run\_unit\_tests\_benchmark".
\end{itemize} 

According to the default settings above, the benchmarking test will only analyze unit tests that run 
500 msecs (ESMF\_BENCHMARK\_THRESHOLD\_MSEC)
or longer. If a unit test runs 3 percent (ESMF\_BENCHMARK\_TOLERANCE) or more beyond the benchmarked
unit test, it will be flagged as failing the benchmark test. The developer may change these parameters
as desired. The following is an example of the output of running "make run\_unit\_tests\_benchmark":

\end{sloppypar}
\begin{verbatim} 



The following unit tests with a threshold of 500 msecs. passed the 3% 
 tolerance benchmark test:

PASS: src/Infrastructure/DELayout/tests/ESMF_DELayoutWorkQueueUTest.F90
PASS: src/Infrastructure/Field/tests/ESMF_FieldCreateGetUTest.F90
PASS: src/Infrastructure/Field/tests/ESMF_FieldRegridCsrvUTest.F90
PASS: src/Infrastructure/Field/tests/ESMF_FieldRegridXGUTest.F90
PASS: src/Infrastructure/Field/tests/ESMF_FieldStressUTest.F90
PASS: src/Infrastructure/TimeMgr/tests/ESMF_CalRangeUTest.F90
PASS: src/Infrastructure/VM/tests/ESMF_VMBarrierUTest.F90
PASS: src/Infrastructure/VM/tests/ESMF_VMUTest.F90
PASS: src/Infrastructure/XGrid/tests/ESMF_XGridMaskingUTest.F90
PASS: src/Infrastructure/XGrid/tests/ESMF_XGridUTest.F90
PASS: src/Superstructure/Component/tests/ESMF_CompTunnelUTest.F90


The following unit tests with a threshold of 500 msecs. failed the 3% 
 tolerance benchmark test:

FAIL: src/Infrastructure/Field/tests/ESMF_FieldRegridUTest.F90
      Test elapsed time: 4331.446 msec.
      Benchmark elapsed time: 2958.47675 msec.
      Increase: 46.41%

FAIL: src/Infrastructure/FieldBundle/tests/ESMF_FieldBundleRegridUTest.F90
      Test elapsed time: 2051.05675 msec.
      Benchmark elapsed time: 1920.42125 msec.
      Increase: 6.8%

FAIL: src/Infrastructure/LogErr/tests/ESMF_LogErrUTest.F90
      Test elapsed time: 2986.40425 msec.
      Benchmark elapsed time: 2583.36775 msec.
      Increase: 15.6%



Found 167 exhaustive multi-processor unit tests files, of those with a 
 threshold of 500 msecs. 11 passed the 3% tolerance benchmark test, and 3 failed.

Benchmark install date: Thu Jun  4 13:26:55 MDT 2015

\end{verbatim}

Note that only the unit tests that have an elapsed time of 500 msecs. or greater are listed. In addition, the date when
the benchmark install was completed is displayed.

\begin{sloppypar}
When a unit test run it benchmarked it is written to a directory such as 
 "BENCHMARKDIR/test/testg/Darwin.gfortran.64.mpich2.default/".
Therefore you can only compare unit tests elapsed between the identical configurations. 

To implement the benchmarking tool, the unit tests were modified to record the elapsed time of each PET.
The stdout file of each unit test has the following lines i.e.
\end{sloppypar}

\begin{verbatim}

ESMF_GridItemUTest.stdout: PET 0 Test Elapsed Time 5.7840000000000007 msec.
ESMF_GridItemUTest.stdout: PET 1 Test Elapsed Time 5.7259999999999982 msec.
ESMF_GridItemUTest.stdout: PET 2 Test Elapsed Time 6.6200000000000010 msec.
ESMF_GridItemUTest.stdout: PET 3 Test Elapsed Time 5.7190000000000021 msec.

\end{verbatim}


The benchmarking tool uses the average of the four elapsed times to determine the test results
since the elapsed times of each PET can vary.


\subsubsection{Examples}
The examples are written to help users understand a specific use of an
ESMF capability. The examples appear as text in the ESMF Reference Manual, therefore
care must be taken to insure that correct portions of the examples appear in the
document. Latex tags have been created to designate which portions of the
examples are visible in the document. 

BOE and EOE are used between text describing the example. BOC and EOC are used between
actual working code that appears in the Reference Manual. Below is an example of how
the tags are used:

\begin{verbatim}

!-------------------------------- Example -----------------------------
!>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%
!BOE
!\subsubsection{Get Grid and Array and other information from a Field}
!\label{sec:field:usage:field_get_default}
!
!  A user can get the internal {\tt ESMF\_Grid} and {\tt ESMF\_Array}
!  from a {\tt ESMF\_Field}.  Note that the user should not issue any destroy command
!  on the retrieved grid or array object since they are referenced
!  from within the {\tt ESMF\_Field}. The retrieved objects should be used
!  in a read-only fashion to query additional information not directly
!  available through the {\tt ESMF\_FieldGet()} interface.
!
!EOE

!BOC
    call ESMF_FieldGet(field, grid=grid, array=array, &
        typekind=typekind, dimCount=dimCount, staggerloc=staggerloc, &
        gridToFieldMap=gridToFieldMap, &
        ungriddedLBound=ungriddedLBound, ungriddedUBound=ungriddedUBound, &
        totalLWidth=totalLWidth, totalUWidth=totalUWidth, &
        name=name, &
        rc=rc)
!EOC
    if(rc .ne. ESMF_SUCCESS) finalrc = ESMF_FAILURE
    print *, "Field Get Grid and Array example returned"

    call ESMF_FieldDestroy(field, rc=rc)
    if(rc .ne. ESMF_SUCCESS) finalrc = ESMF_FAILURE
!>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%>%

\end{verbatim}

Note that any code or text that is not contained within the tag pairs does not appear in the
Reference Manual.

\begin{sloppypar}
Most examples can be run on multiple processors or a single processor. Those examples should
have the tag, "ESMF\_EXAMPLE" as a comment in the body of the example file. If the example
can only run on multiple processors then use the tag, "ESMF\_MULTI\_PROC\_EXAMPLE".

\paragraph{Disabling examples}
When an example is removed from the makefile, the "ESMF\_EXAMPLE" or 
"ESMF\_MULTI\_PROC\_EXAMPLE"
tags must be modified so that the example is not flagged as failed. 
The recommended way to modify these tags is to replace the first underscore with "\_disable\_", 
thus "ESMF\_EXAMPLE" becomes
"ESMF\_disable\_EXAMPLE".
\end{sloppypar}

\subsubsection{System Tests}

System tests are written to test functionality that spans several
classes.  The following areas should be addressed in system testing.

\begin{itemize}
\item Design omissions (e.g. incomplete or incorrect behaviors).
\item Associations between objects (e.g. fields, grids, bundles).
\item Control and infrastructure. (e.g. couplers, time management, error handling).
\item Feature interactions or side effects when multiple features are used
simultaneously.
\end{itemize}

The system tester should issue a test log after each software release is tested,
which is recorded on the {\bf Test and Validation} web page. The test
log shall
include: a test ID number, a software release ID number, testing environment
descriptions, a list of test cases executed, results, and any unexpected
events.

\paragraph{Writing System Tests}

System tests should contain the following sections:
\begin{itemize}
\item Create - Create Components, Couplers, Clock, Grids, States etc.
\item Register - Register Components and the initialize, run and finalize subroutines.
\item Initialize - Initialize as needed.
\item Run - Run the test.
\item Finalize - Verify results.
\item Destroy - Destory all classes.
\end{itemize}

Most system tests can be run on multiple processors or a single processor. Those system tests should
have the tag, "ESMF\_SYSTEM\_TEST" as a comment in the body of the system test. If the system test
can only run on multiple processors then use the tag, "ESMF\_MULTI\_PROC\_SYSTEM\_TEST".

At the end of the system it is recommended that the ESMF\_TestGlobal subroutine be used to gather
test results from all processors and print out a single PASS/FAIL message instead
of individual PASS/FAIL messages from all the processors.
After the test is written it must be documented on the ESMF Test \& Validation
web page:

\begin{verbatim}

http://www.earthsystemmodeling.org/developers/test/system/

\end{verbatim}


\begin{sloppypar}
\paragraph{Disabling system tests}
When a system test is removed from the makefile, the "ESMF\_SYSTEM\_TEST" or 
"ESMF\_MULTI\_PROC\_SYSTEM\_TEST"
tags must be modified so that the system test is not counted as failed. 
The recommended way to modify these tags is to replace the first underscore with "\_disable\_", thus 
ESMF\_SYSTEM\_TEST becomes 
ESMF\_disable\_SYSTEM\_TEST.
\end{sloppypar}

\subsubsection{Test Harness}
The Test Harness is a highly configurable test control system for conducting 
thorough testing of the Regridding and Redistribution processes. The Test Harness 
consists of a single shared executable and a collection of customizable resource
files that define an ensemble of test configurations tailored to each ESMF class.
The Test Harness is integrated into the Unit test framework, enabling
the Test Harness to be built and run as part of the Unit tests. The test results 
are reported to a single standard-out file which is located with the unit test 
results.

See section~\ref{sec:harness} for a complete discussion of the test harness.

% the test harness documentation is found in /src/test_harness/doc

\paragraph{Analyzing Test Harness results }
When the Test Harness completes a run, the results from the ensemble of tests are
reported in two ways. The first is analogous to the unit test reporting, since the
test harness is run as part of the unit tests, a summary of the results are recorded 
just as with the unit tests. In addition to the standard unit test reporting, the
test harness is also able to produce a human readable report. The report consists 
of a concise summary of the test configuration along with the test results. The test 
configuration is described in terms of the Field Taxonomy syntax and user provided 
strings. The intent is not to provide a exhaustive description of the test, but 
rather to provide a useful description of the failed tests.

Consider another example similar to the previous one, where two descriptor strings 
describing an ensemble regridding tests. The first uses the patch method and the 
second uses bilinear interpolation.

\begin{center}
\begin{verbatim}
[ B1 G1; B2 G2 ] =P=> [ B1 G1; B2 G2 ] 
[ B1 G1; B2 G2 ] =B=> [ B1 G1; B2 G2 ] 
\end{verbatim}
\end{center}

Suppose the associated specifier files indicate that the source  grid is rectilinear
and is 100 X 50 in size. The destination grid is also rectilinear and is 80 X 20 
in size. 
Both grids are block distributed in two 
ways, 1 X NPETS and NPETS X 1. And suppose that the first dimension of both the
source and destination grids are periodic. If the test succeeds for the bilinear
regridding, but fails for one of the patch regridding configurations, the reported results
could look something like

\begin{verbatim}
SUCCESS: [B1 G1; B2 G2 ] =B=> [B1 G1; B2 G2 ] 
FAILURE: [B1{1} G1{100}+P; B2{npets} G2{50} ] =P=> [B1{1} G1{80}+P; B2{npets} G2{20} ] 
     failure at line 101 of test.F90
SUCCESS: [ B1{npets} G1{100} +P; B2{1} G2{50} ] =P=> [ B1{npets} G1{80}+P; B2{1} G2{20} ] 
\end{verbatim}

The report indicates that all the test configurations for the bilinear regridding
are successful. This is indicated by the key word SUCCESS which is followed by the 
successful problem descriptor string. Since all of the tests in the first case pass,
there is no need to include any of the specifier information. For the second
ensemble of tests, one configuration passed, while the other failed. In this case,
since there is a mixture of successes and failures, the report includes 
specifier information for all configurations to help indicate the source of the
test failure. The supplemental information, while not a complete problem description
since it lacks items such as the physical coordinates of the grid and the nature of 
the test field, includes information crucial to isolating the failed test.


\subsubsection{Use Test Cases (UTCs)}

Use Test Cases are problems of realistic size created to test the ESMF
software.  They were initiated when the ESMF team and its users saw that
often ESMF capabilities could pass simple system tests but would fail
out in the field, for real customer problems.  UTCs have realistic
processor counts, data set sizes, and grid and data array sizes.  UTCs are
listed on the {\bf Test \& Validation} page of the ESMF website.  They
are not distributed with the ESMF software; instead they are stored in
a separate module in the main repository called {\tt use\_test\_cases}.

\subsubsection{Beta Testing}

ESMF software is released in a beta form, as an Internal Release,
three months before it is publicly released.  This gives users
a chance to test the software and report back any problems to 
support.

\subsubsection{Automated Regression Tests}

The purpose of regression testing is to reveal faults caused by new
or modified software (e.g. side effects, incompatibility between 
releases, and bad bug fixes).  
Regression tests regularly exercise all interfaces of the code on 
all target platforms.  
The regression test results for the last two weeks can be found 
\htmladdnormallink{{\it here.}}{http://www.earthsystemmodeling.org/developers/test/daily/2week\_results/trunk\_results\_noheader/platform\_sort.shtml}
This web page provides a complete color-coded current view of the state of the trunk ESMF software, sorting options by platform or compiler are provided.
A similiar test results web page for the branch is also available.
Clicking on any of the cells will display the specific test report for that day.
Hovering over the test name i.e., Blues gfortran, will reveal notes particular to that platform/compiler. Clicking on the test name, will take you to the home page of the platform. 

The platforms that run the regression tests, email the test results to a server that updates the test results web page. A script checks for test reports every 15 minutes, and updates the web page. The time of the last update appears on the web page.

\subsubsection{Investigating Test Failures}
The regression test results web
\htmladdnormallink{{\it page}}{http://www.earthsystemmodeling.org/developers/test/daily/2week\_results/trunk\_results\_noheader/platform\_sort.shtml}
 provides a color-coded view of the state of the software.
When a developer finds that a test fails on a particular platform with a particular compiler, sometimes the bug is readily identified and fixed.
However other times the developer may want to know if the test fails on other platforms and if the failure is related to a compiler, mpi configuration or optimized/debug execution.
The developer would need to click to all the cells of different platforms searching for the test results for that particular test.

A tool was created to allow the developers to query the test results for a specific test for a specific date, as long as it is within two weeks of the current date.
The developer may send a query test results message to the following email address: esmftest@cgd.ucar.edu
The subject of the email must be exactly "Test\_Results\_Query". The body of the email message must be "Test:" followed by the test name and "Date" followed by the desired date. The format must be a three letter month and a number. If the date is 2 digits, greater than 9, then insert one space between the month and date e.g. Apr 25. If the day is a single digit insert two spaces, between the month and day e.g. Apr  4.

\begin{verbatim}
Test:ESMF_FieldBundleSMMUTest.F90
Date:Feb  8
   or
Date Feb 28
\end{verbatim}

This mail box is checked every quarter hour on the quarter hour, the results are emailed to:esmf\_test@cgd.ucar.edu.
The subject of the results email for this example would be:
\begin{verbatim}

        ESMF_FieldBundleSMMUTest.F90 test results for Feb  8

\end{verbatim}

The body of the email would be as follows:

\begin{verbatim}

	ESMF_Blues_PGI:PASS: mvapich2/g: src/Infrastructure/FieldBundle/tests/ESMF_FieldBundleSMMUTest.F90
	ESMF_Blues_PGI:PASS: mvapich2/O: src/Infrastructure/FieldBundle/tests/ESMF_FieldBundleSMMUTest.F90
	ESMF_Blues_PGI:CRASHED: mpich3/g: src/Infrastructure/FieldBundle/tests/ESMF_FieldBundleSMMUTest.F90
	ESMF_Blues_PGI:PASS: mpich3/O: src/Infrastructure/FieldBundle/tests/ESMF_FieldBundleSMMUTest.F90
	ESMF_Blues_PGI:PASS: openmpi/g: src/Infrastructure/FieldBundle/tests/ESMF_FieldBundleSMMUTest.F90
	ESMF_Blues_PGI:PASS: openmpi/O: src/Infrastructure/FieldBundle/tests/ESMF_FieldBundleSMMUTest.F90
	ESMF_Discover_g95:PASS: mvapich2/g: src/Infrastructure/FieldBundle/tests/ESMF_FieldBundleSMMUTest.F90
	ESMF_Discover_g95:PASS: mvapich2/O: src/Infrastructure/FieldBundle/tests/ESMF_FieldBundleSMMUTest.F90
	ESMF_Haumea_g95:PASS: mpich2/g: src/Infrastructure/FieldBundle/tests/ESMF_FieldBundleSMMUTest.F90
	ESMF_Haumea_g95:PASS: mpich2/O: src/Infrastructure/FieldBundle/tests/ESMF_FieldBundleSMMUTest.F90
	ESMF_Haumea_g95:PASS: mvapich2/g: src/Infrastructure/FieldBundle/tests/ESMF_FieldBundleSMMUTest.F90
	ESMF_Haumea_g95:PASS: mvapich2/O: src/Infrastructure/FieldBundle/tests/ESMF_FieldBundleSMMUTest.F90
	ESMF_Pluto_g95:FAIL: mpich2/g: src/Infrastructure/FieldBundle/tests/ESMF_FieldBundleSMMUTest.F90
	ESMF_Pluto_g95:FAIL: mpich2/O: src/Infrastructure/FieldBundle/tests/ESMF_FieldBundleSMMUTest.F90
	ESMF_Pluto_g95:FAIL: mvapich2/g: src/Infrastructure/FieldBundle/tests/ESMF_FieldBundleSMMUTest.F90
	ESMF_Pluto_g95:FAIL: mvapich2/O: src/Infrastructure/FieldBundle/tests/ESMF_FieldBundleSMMUTest.F90

\end{verbatim}

Note that if the date of the query is the current day, the developer should query periodically during the day since
the test results are being updated as platforms report their test results.
If a test crashes it can be because another test hung and the test in question did not run.

Another instance where this tool is useful is when a developer adds a new test, after the nightly tests run, the developer can run a query to quickly see the test results.

\subsubsection{Building the Documentation}

As software development progresses, the documentation is updated, built and posted at
\htmladdnormallink{{\it http://earthsystemmodeling.org/docs/nightly/develop/dev_guide/}}{http://earthsystemmodeling.org/docs/nightly/develop/dev_guide/}

The documents are built daily in the early morning, the results of the builds are posted at
\htmladdnormallink{{\it http://earthsystemmodeling.org/doc/}}{http://earthsystemmodeling.org/doc/}

These documents can be updated by the developers, by checking out the documents from the repository and submitting the edited files. To have the new version of the documents posted on the web, the developer must sent a request to the following email address: esmftest@cgd.ucar.edu. The subject of the email indicates which document to build and post.
The following is the list of subjects that have been implemented:

\begin{itemize} 
\item Build\_Dev\_Guide\_Doc
\item Build\_NUOPC\_Doc
\item Build\_Ref\_Doc
\item Build\_ESMPy\_Doc
\item Build\_CICE\_NUOPC\_CAP\_Doc
\item Build\_HYCOM\_NUOPC\_CAP\_Doc
\item Build\_LIS\_NUOPC\_CAP\_Doc
\item Build\_MOM\_NUOPC\_CAP\_Doc
\item Build\_WRFHYRO\_NUOPC\_CAP\_Doc
\end{itemize}

\subsubsection{Testing for Releases}

We provide two types of tar files, the ESMF source and the shared
libraries of the supported platforms. Consequently, there are two test
procedures followed before placing the tar files on the ESMF download website. 

The {\bf Source Code Test Procedure} is followed on all the supported
platforms for the particular release.

\begin{enumerate}
\item Verify that the source code builds in both {\tt BOPT=g} and {\tt BOPT=O}.
\item Verify that  the {\tt ESMF\_COUPLED\_FLOW} demonstration executes successfully.
\item Verify that the unit tests run successfully, and that there are no {\tt NON-EXHAUSTIVE} unit tests  failures.
\item Verify that all system tests run successfully. 
\end{enumerate}

The {\bf Shared Libraries Test Procedure} is also followed on all supported
platforms for a release.

\begin{enumerate}
\item Change to the {\tt CoupledFlowEx} directory and execute {\tt make}. Verify that the demo runs successfully.
\item Change to the {\tt CoupledFlowSrc} directory and execute {\tt make} then {\tt make run}. Verify that the demo runs successfully.
\item Change to the {\tt examples} directory and execute {\tt make} and {\tt make run}. Verify that the example runs successfully.
\end{enumerate}














