% $Id$

\section{Overview of Distributed Data Methods}

FieldBundles, Fields, and Arrays all have versions of the following
data communication methods.  In these objects, data is communicated 
between DEs.  Depending on the underlying communication 
mechanism, this may translate within the framework to a data 
copy, an MPI call, or something else.  
The ESMF goal of providing
performance portability means the framework will in the future
attempt to select the
fastest communication strategy on each hardware platform transparently 
to the user code.  (The current implementation uses MPI for communication.)

Communication patterns, meaning exactly which bytes need to be copied 
or sent from one PET to another to perform the requested operation,
can be precomputed during an initialization phase and then later 
executed repeatedly.
There is a common object handle, an {\tt ESMF\_RouteHandle}, which
identifies these stored communication patterns. 
Only the {\tt ESMF\_RouteHandle} and the source and destination 
data pointers must be supplied at runtime to minimize execution overhead.

\subsection{Higher Level Functions}
The following three methods are intended to map closely to 
needs of applications programs.  They represent higher level
communications and are described in more detail in the following
sections.  They are:

\begin{itemize}

\item {\bf Halo}
Update ghost-cell or halo regions at the boundaries
of a local data decomposition.
\item {\bf Regrid}
Transform data from one Grid to another, performing
any necessary data interpolation.
\item {\bf Redist}
Copy data associated with a single Grid from
one decomposition to another.  No data interpolation is necessary.

\end{itemize}

\subsection{Lower Level Functions}
The following methods correspond closely to the lower level
MPI communications primitives.  They are:

\begin{itemize}

\item {\bf Gather}
Reassembling data which is decomposed over a set of DEs into a single
block of data on one DE.
\item {\bf AllGather}
Reassembling data which is decomposed over a set of DEs into multiple
copies of a single block of data, one copy per original DE.
\item {\bf Scatter}
Spreading an undercomposed block of data on one DE over a set of DEs,
decomposing that single block into smaller subsets of data, one
data decomposition per DE.
\item {\bf AlltoAll}
Spreading an undercomposed block of data from multiple DEs onto
each of the other DEs in the set, resulting in a set of multiple decomposed 
data blocks per DE, one from each of the original source DEs.
\item {\bf Broadcast}
Spreading an undecomposed block of data from one DE onto all other
DEs, where the resulting data is still undecomposed and simply
copied to all other DEs.
\item {\bf Reduction}
Computing a single data value, e.g. the data maximum, minimum, sum, etc
from a group of decomposed data blocks across a set of DEs, where the
result is delivered to a single DE.
\item {\bf AllReduce}
Computing a single data value, e.g. the data maximum, minimum, sum, etc
from a group of decomposed data blocks across a set of DEs, where the
result is delivered to all DEs in the set.

\end{itemize}

\subsection{Common Options}
\label{sec:routeoptions}

ESMF will select an appropriate default for the
internal communication strategy for executing the communications.  
However, additional control is available
to the user by specifying the following route options.
(For more details on exactly what changes with the various options,
see Section \ref{sec:routeimpl}.)

\subsection{Design and Implementation Notes}
\label{sec:routeimpl}

\begin{enumerate}

\item

\begin{sloppypar}
There is an internal {\tt ESMC\_Route} class which supports the 
distributed communication methods.  There are 4 additional internal-only
classes which support {\tt ESMC\_Route}: {\tt ESMC\_AxisIndex}, 
{\tt ESMC\_XPacket}, {\tt ESMC\_CommTable}, and {\tt ESMC\_RTable};
and a public {\tt ESMF\_RouteHandle} class which is what the user 
sets and gets.  The implementation is in C++, with interfaces in Fortran 90.
\end{sloppypar}

The general communication strategy is that each
DE computes its own communication information independently,
in parallel, and adds entries to a per-PET route table
which contains all needed sends and receives (or gets and puts) 
stored in terms relative to itself.  (Implementation note: this
code will need to be made thread-safe if multiple threads are
trying to add information to the same route table.)

AxisIndex is a small helper class which contains an index minimum
and maximum for each dimension and is used to describe an n-dimensional
hypercube of information in index space.  These are associated with 
logically rectangular grids and local data arrays.  
There are usually multiple instances of them, for example the local
data chunk, and the overall global index-space grid this data is
a subset of.  Within each of the local or global categories, there are
also multiple instances to describe the allocated space, the total area,
the computational area, and the exclusive area.
%%%
%%\begin{description}
%%\item [allocated space] all memory associated with this data block
%%\item [total area] subset of allocated area which includes halo regions 
%%but not unused space allocated simply for padding or alignment reasons
%%\item [computational area] subset of total area which includes all data 
%%which is computed, excludes halo regions 
%%\item [exclusive area] subset of computational area data which is not 
%%read for halo updates
%%\end{description}
%%%
(Implementation note: the allocated space is only partially implemented
internally and has no external user API yet.)

An Exchange Packet (XPacket) describes groups of memory addresses
which constitute an n-dimensional hypercube of data.
Each XPacket has an offset from a base address, 
a contiguous run length, 
a stride (or number of items to skip) per dimension,
and a repeat count per dimension. 
See Figure \ref{fig:xpacketbasic} for a diagram of how the XPacket
describes memory.
The actual unit size stored in an XPacket is an item count, 
so before using an XPacket to address bytes of memory
the item size must be known and the
counts multiplied by the number of bytes per item.  This allows
the same XPacket to describe different data types which have the
same memory layout, for example 4 byte integers and 8 byte reals/doubles.
The XPacket methods include basic set/get, how to turn
a list of AxisIndex objects into an XPacket, compute a local XPacket from one
in global (undecomposed grid) space, and a method to compute the intersection
of 2 XPackets and produce a 3rd XPacket describing that region.  

\begin{center}
\begin{figure}
\scalebox{0.8}{\includegraphics{Basic_xpacket}}
\caption{How an Exchange Packet (XPacket) describes the memory
layout for a rectangular hypercube of data.}
\label{fig:xpacketbasic}
\end{figure}
\end{center}

The Communication Table (CommTable) class encapsulates which other PETs this
PET needs to talk to, and in what order.  There are create and destroy
methods, methods to set that a PET has data either to
send or receive, and query routines that return an answer
to the question 'which PET should I exchange data with next'.  

The Route Table (RTable) class contains a list of
XPackets to be sent and received from other PETs.
It has create/destroy methods, methods to add XPackets to the list for 
each PET, and methods to retrieve the XPackets from any list.

The top level class is a Route.  A Route object contains a send RTable, 
a recv RTable, a CommTable, and a pointer to a Virtual Machine.   
The VM must include all PETs which are participating
in this communication.
The Route methods
include create/destroy, setting a send or recv XPacket
for a particular PET,
and some higher level functions specific to each
type of communication, for example RoutePrecomputeHalo
or RoutePrecomputeRedist.  These latter functions
are where the XPackets are actually computed and added to
the Route table.  Each DE computes its own set of intersections,
either source or destination, and fills its own corresponding PET table.
The Route methods also include a RouteRun method which executes the code
which actually traverses the table and sends the information between PETs.

A RouteHandle class is a small helper class which is returned through
the public API to the user when a Route is created, and passed back in
through the API to select which precomputed Route is to be executed.
A RouteHandle contains a handle type and a pointer to a Route object.
In addition, for use only by the Regrid code, there is an additional Route
pointer and a TransformValues pointer.  (TransformValues is an internal
class only used by the Regridding code.)  If the RouteHandle describes the
Route for a FieldBundle, then the RouteHandle can contain a list of Routes,
one for each Field in the FieldBundle, and for Regrid use, a list of additional
Routes instead of a single Route.  There is also a flag to indicate whether
a single Route is applicable to all Fields in a FieldBundle or whether there
are multiple Routes.
The RouteHandle methods are fairly basic; mostly accessor methods
for getting and setting values.


\item

While intended for any distributed data communication method,
the current implementation only builds a Route object for
the halo, redist, and regrid methods.  Scatter, Gather,
AllGather, and AlltoAll 
should have the option of building a Route for operations
which are executed repeatedly.  This should only require
writing a Precompute method for each one; the existing 
RouteRun can be invoked for these operations.
(This is a lack-of-implementation-time issue, not a design
or architecture issue.)

\item

The original design included automatic detection of different
Routes and internal caching, so the user API did not have to
include a RouteHandle object to identify which Route was
being invoked.  However, users requested that the framework
not cache and that explicit RouteHandle arguments be created
and required to invoke the distributed data methods.
Nothing prevents this code from being revived from the CVS
repository and reinstated in the system, should automatic
caching be desired by future users.

\item

The current distributed methods have 2 related but distinct
interfaces which differ in what information they require
and whether they use RouteHandles:

\begin{enumerate}
\item[Precompute/Run/Release]
This is the most frequently used interface set.
It contains 3 distinct phases: precomputing which bytes must
be moved, actually executing the communications operation,
and releasing the stored information.  This is intended for
any communication pattern which will be executed more than once.
\item[All-in-One]
For a communication which will only be executed once, or in
any situation in which the user does not want to save a RouteHandle,
there are interfaces which do not have RouteHandles as part of
the argument list.  Internally the code computes a Route,
executes it, and releases the resources before returning.
\end{enumerate}

\item

The current CommTable code executes one very specific communication
strategy based on input from a user who did extensive timing
measurements on several different hardware platforms.  Rather than
broadcasting all data at once asynchronously, it selects combinations
of pairs of processors and has them execute a SendRecv operation, which
does both a data send and a data receive in a single call.
At each step in the execution, different pairs of processors
exchange data until all pair combinations have been selected.

The table itself must be a power of 2 in size; the number of
PETs is rounded up to the next power of 2 and then all entries
for PETs larger than the actual number are marked as no-ops.

There are many alternative execution strategies, including a
completely asynchronous execution, in numeric PET order, without
computing processor pairs.  Also single-direction communications 
are possible (only the Send XPackets are processed, or only
the Receive XPackets) in either a synchronous or asynchronous mode.  
This would not require any changes to the XPacket or RTable classes,
but would require writing a set of alternative RouteRun methods.

\item

The current RouteRun routine has many possible performance options for how
to make the trade off between time spent packing disjoint memory
blocks into a single buffer to minimize the number of sends,
verses simply sending the contiguous blocks without the pack overhead.
The trade offs are not expected to be the same on all systems;
hardware latency verses bandwidth characteristics will differ,
plus the underlying communication software (MPI, shared memory, etc)
will change the performance.  Also the size of the data blocks
to be sent, the amount of contiguity, and limits on the number 
of outstanding communication buffers all affect what options are best.

The {\tt ESMF\_RouteOptions} are listed in \ref{sec:routeoptions}; 
the following description contains more implementation detail 
about what each of the options
controls inside the execution of a Route.  Note that the options
do not affect the creation of a Route, nor any of the Precompute
code, and can optionally be changed each time the Route is run.

Packing options:
\begin{enumerate}
\item[By Buffer]
If multiple memory addresses are provided to RouteRun (from
bundle-level communications, for example), then this option
packs data across all buffers/blocks as specified by the other
packing flags before sending or receiving.
Note: unlike the other packing flags, this is handled in the
code at a higher level by either passing down multiple addresses
into the route run routine or not.  If multiple addresses are
passed into the run routine, they will be packed.  The "no-packing"
option at this level would be identical to looping at the outermost
level in the RouteRun code and therefore there is no disadvantage
to calling this routine once per address (and the advantage is
not adding yet another coding loop inside the already complex
RouteRun code).  The higher level list-of-address code can be
disabled by clearing this flag (which is on by default).
\item[By PET]
All data from a single block
intended for a remote PET is packed into a single send
buffer, and sent in a single VM communications call.  
A buffer large enough to receive all data 
coming from that remote PET is allocated, the data is received,
and then the data is copied into the final location.
See \ref{fig:routepackall}.
\item[By XP]
All data described by a single XPacket (which is a n-dimensional
hyperslab of memory) is packed into a single buffer for sending,
and a single buffer large enough to receive an XPacket is 
allocated for receiving the data.
See \ref{fig:routepack}.
\item[No Packing]
A VM communication call is made for each single contiguous strip
of memory, regardless of how long or short.
\item[MPI Vector]
MPI implements a set of interfaces for sending and receiving which
allows certain strided memory patterns to be sent in a single call.
The actual implementation is up to the MPI library itself.  But no
user-level data copy is needed in this case. (Not implemented yet.)
\end{enumerate}
Note that in all packing options, if the XPacket describes a
chunk of memory which is completely contiguous, then the code
does not allocate a packing or unpacking buffer but supplies the
actual data address to the communications call so the data is
read or written in place.

\begin{center}
\begin{figure}
\scalebox{0.9}{\includegraphics{Basic_xpacket_pack}}
\caption{A common XPacket pattern which generally benefits from
packing; the overlap region between 2 DEs during a halo update
are often short in the contiguous dimension and have a high 
repeat count.}
\label{fig:xpacketpack}
\end{figure}
\end{center}

\begin{center}
\begin{figure}
\scalebox{0.8}{\includegraphics{Basic_xpacket_2up}}
\caption{When there are multiple XPackets destined for the
same remote PET there are more options for how to order the
contiguous pieces into a packed buffer.}
\label{fig:xpacket2up}
\end{figure}
\end{center}

\begin{center}
\begin{figure}
\caption{When the XPacket describes memory which is physically
a single contiguous region, there is no need to copy the data
into another buffer; it can be communicated in place.  There is
a flag in the XPacket which marks how many of the dimensions
are contiguous.}
\label{fig:xpacketcontig}
\scalebox{0.8}{\includegraphics{Basic_xpacket_contig}}
\end{figure}
\end{center}

\begin{center}
\begin{figure}
\scalebox{0.9}{\includegraphics{Basic_pack_route}}
\caption{Often the overhead of making multiple communication calls 
outweighs the cost of copying non-contiguous data into a contiguous buffer,
sending it in a single operation, and then copying it to the final
memory locations on the receiving side.}
\label{fig:routepack}
\end{figure}
\end{center}

\begin{center}
\begin{figure}
\scalebox{0.8}{\includegraphics{Basic_2up_route}}
\caption{Once there is more than a single XPacket to pack, there
are many more interleave options.  For example, packing in the
order: 1, 4, 2, 5, 3, 6 would also be possible here.  However
the code becomes more complicated when the XPackets have different
repeat counts, and has no real performance advantage over the
straightforward packing of each XPacket in sequence.  Note that
this packing is the same whether it refers to multiple XPackets
from the same memory buffer or from multiple buffers.}
\label{fig:routepackall}
\end{figure}
\end{center}

The following options refer to the internal strategy for 
executing the route and not to whether the user-level API call
returns before the route has finished executing.  The current
system only implements user-synchronous calls; asynchronous calls
are on the to-be-written list.

\begin{enumerate}

\item[Sync] Each pair of processors exchanges data with the VM
equivalent of an MPI\_SendRecv() call, which does not return until
both the send and receive have completed.

\item[Async] Each processor executes both an asynchronous send
and asynchronous receive to the other processor and does not wait
for completion before moving on to the next communication in the
CommTable.  Then in a separate loop through the RTables, each
call is waited for in turn and when all outstanding communication
calls have completed, then the API call returns to the user.

\end{enumerate}
(Note that in the Async case it makes much more sense to iterate
through the Route table in PET order instead of the complication
of computing communication pairs and iterating in a non-sequential
order.  The code is as it is now for reasons of implementation speed
and not for any other design reason.  This would require a slightly
simpler, but separate, version of the RouteRun() subroutine.)

\item

FieldBundle-level communication calls have additional packing options
under certain circumstances.  FieldBundles are groups of Fields which
share the same Grid, but they are not required to share the same
data types, data ranks, nor relative data locations.  FieldBundles
in which these things are the same in all Fields are marked inside the
bundle code as being {\bf congruent}.  At communication store time
FieldBundles which have congruent data in all the Fields have the option
of packing all Field data together into fewer communication calls
which generally is expected to give better performance.   
Fields where the data is not of the same type or perhaps not
the same number of items (e.g. different rank, vertex-centered data
vs. cell centered data) can in theory also be packed but in fact
the code becomes more complicated, and in the case of differing
data types may cause system errors because of accessing data
on non-standard byte offsets or putting mixing integer data
with floating data and causing NaN (not a number) exceptions.
In this case, the conservative implementation strategy is to construct 
a separate Route object for each Field, all enclosed in the same
RouteHandle.  Inside the FieldBundle communication code the execution
for both types of FieldBundles is identical for the caller, but inside
the congruent FieldBundle code calls the {\tt ESMF\_RouteRun()} code
once and all communication for all Fields in the FieldBundle is done
when it returns.  The non-congruent FieldBundles execute a separate
{\tt ESMF\_RouteRun()} call for each Field and return to the user 
when all Field data have been sent/received.

There are comments in the code for an intermediate level of
optimization in which the FieldBundle code determines the smallest
number of unique types of Fields in the FieldBundle, and all same types
share the same Route object, but this has not been implemented
at this time.  Once the existing code has been in use for a while,
whether this is useful or needed may become more clear.


\item

The precompute code for all operations must have enough
information to compute which parts of the data arrays
are expected to be sent to remote PETs and also what
remote data is expected to be received by this PET.

These computations depend heavily on what type of distributed
method is being executed.  The regridding methods are described
in detail separately in the Regrid Design and Implementation Notes
section.  The halo and redistribution operations are described here.

\begin{enumerate}

\item[Halo] The total array area, which includes any halo regions,
are intersected with the computational
area of other DEs. The overlap regions are converted from index
space into memory space and stored as XPackets in the RTables.
This code must be aware of: whether the grid was defined as
periodic in any or all of the dimensions since that affects
which halo regions overlap at the grid edges; if the data
is only decomposed into a single block in any dimension (which
means it halos with itself); and if the halo region is large
enough that a halo operation may require intersection with
the N+1 neighbor in any dimension.  

\item[Redistribute]  Each DE computes the overlap
between its own computational region and all DEs in the 
remote Grid, again only working in computational area.  
The overlap regions are converted from index
space into memory space and stored as XPackets in the RTables.
After execution a redistribution, a halo operation may be required
to populate any halo regions with consistent data.

\end{enumerate}
(Note: the Redistribution code has been reimplemented to intersect
the DEs in index space and then convert the overlap region to an XPacket
representation.  Halo still converts the regions from AxisIndex to 
XPackets and then intersects the XPackets, but this code needs to be
changed to intersect in AxisIndex space and once the overlap is computed
then convert to XPackets.  Intersecting AxisIndex objects is very 
much simpler, both to understand and to execute, and more easily 
extensible to multiple dimensions than intersecting XPackets.)

\end{enumerate}

\subsection{Object Model}

The following is a simplified UML diagram showing the structure of the public
RouteHandle class.  See Appendix A, {\it A Brief Introduction to UML}, for a
translation table that lists the symbols in the diagram and their meaning.

\begin{center}
\includegraphics{RouteHandle_obj}
\end{center}


\newpage





